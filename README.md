The independent study on geocoding and mapping historical texts provided valuable research experience and helped develop new data science and data mining skills. It provided an enriching experience in text analysis and data visualization. The initial goals were to scrape a database of texts from HathiTrust, extract location entities, geocode them, and visualize the results on an interactive map. These goals were accomplished, albeit with some twists and turns along the way. I created a HathiTrust work set consisting of five volumes of Harry Potter books in English along with supplementary materials analyzing the cultural and scientific aspects. I then ran the Named Entity Recognition algorithm to extract locations and organizations. There were some challenges in actually getting the algorithm to run to completion, requiring troubleshooting with my mentor. Ultimately we narrowed the input to 5 volumes which returned results. I loaded this CSV output into a Pandas DataFrame, keeping just the "Location" and "Organization" labeled entities. Using the Nominatim and Folium libraries, I iteratively geocoded each entity and mapped the resulting geographic coordinates.
Having some prior coding experience in Python and Jupyter Notebooks, I expected a relatively smooth process. However, there were definite learning curves, especially regarding new concepts like web scraping, entity extraction, and geocoding APIs. For example, navigating the HathiTrust website and understanding how to query and extract data properly took considerable reading and experimentation. Similarly, working with a large volume of text caused some slowdowns. Furthermore, choosing the right geocoding library and handling tricky geocoded coordinates took some debugging and troubleshooting. 
The corpus focused on Harry Potter, given the interesting opportunity to map both real and fictional locations. The visualization shows clusters of markers in the UK, representing places like London. There are also standalone markers plotted across the globe marking real locations mentioned. Among these, a handful of markers hover over the Atlantic Ocean or other remote areas - likely indicating a fictional entity that could not be properly geocoded. On a technical level, working with a large corpus of text also posed challenges. Simple operations like filtering the data frame took a long time due to the size of the data. This required optimizations like only operating on subset samples. Such computational restraints were unforeseen originally but taught valuable lessons.


The project timeline was appropriate but ambitious. Most weekly milestones were achieved, but illnesses and exam conflicts caused occasional delays, highlighting the need for padding. That said, diving deeper into coding and mapping opened up new interests in data visualization and text analysis that could drive future projects or facets of my career. This independent study enriched my research acumen and toolset as an aspiring data scientist. The exposure to real-world APIs, new libraries, and impactful visualization cemented foundational skills and knowledge from coursework. It also demonstrated the iterative, puzzling, rewarding nature of following curiosity into new techniques and technologies according to a goal. Such experience and versatile data manipulation, analysis, and communication abilities will undoubtedly empower future research endeavors and roles. By completing this project, I gained valuable skills in distributed text analysis at scale as well as applied data science techniques like entity extraction, geocoding, and interactive mapping. The experience working with real-world unstructured data and developing a complete notebook demonstration will no doubt help in future research and roles leveraging these capabilities. My supervisor was always available when I encountered issues and provided useful documentation and examples that helped further my understanding.

